{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-models.ipynb\n",
    "## Workflow: Slot-level clustering → Demand models → Price-response grids → Save artifacts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Setup & Data Loading\n",
    "# 1. Parameters & Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (800282122.py, line 37)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mSCRIPT_DIR = os.path.dirname(os.path.realpath(\"C:\\Users\\marti\\OneDrive\\Documentos\\Martim\\Académico\\Master LBS\\Clases MAM2025\\LondonLAB\\Clays_LondonLAB_Project\"))\u001b[39m\n                                                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, PoissonRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor # Added as another strong baseline/alternative\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Calibration & Explainability\n",
    "# from sklearn.calibration import calibration_curve # For classification\n",
    "import shap\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Optuna for hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "# Disable Optuna's verbose logging during trials, but enable it for study summary\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Get the directory of the current script (code/)\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(\"C:\\Users\\marti\\OneDrive\\Documentos\\Martim\\Académico\\Master LBS\\Clases MAM2025\\LondonLAB\\Clays_LondonLAB_Project\"))\n",
    "\n",
    "# Get the project root directory (CLAYS_LONDONLAB_Project/)\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, os.pardir))\n",
    "\n",
    "\n",
    "# Define Parameters\n",
    "SLOT_PARQUET = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"slot_level.parquet\")\n",
    "CLUSTER_CSV = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"time_slot_clusters.csv\")\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\") # This matches your structure for a top-level 'models' folder\n",
    "GRID_PATH = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"price_response_grid.csv\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(\"../data/processed/\", exist_ok=True) # Ensure this exists too\n",
    "\n",
    "\n",
    "# Print paths for verification (paths are now absolute)\n",
    "print(\"Libraries imported and parameters set.\")\n",
    "print(f\"Script directory: {SCRIPT_DIR}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Models will be saved in: {MODELS_DIR}\")\n",
    "print(f\"Cluster CSV will be saved to: {CLUSTER_CSV}\")\n",
    "print(f\"Price Grid CSV will be saved to: {GRID_PATH}\")\n",
    "print(f\"Loading slot data from: {SLOT_PARQUET}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Still cannot find the file at the constructed path: C:\\Users\\marti\\OneDrive\\Documentos\\Martim\\Académico\\Master LBS\\Clases MAM2025\\LondonLAB\\data\\processed\\slot_level.parquet\n",
      "Please double-check that the file exists and the path logic is correct for your environment.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\marti\\\\OneDrive\\\\Documentos\\\\Martim\\\\Académico\\\\Master LBS\\\\Clases MAM2025\\\\LondonLAB\\\\data\\\\processed\\\\slot_level.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     df_slot = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSLOT_PARQUET\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mERROR: Still cannot find the file at the constructed path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSLOT_PARQUET\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    665\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:267\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    265\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    275\u001b[39m         path_or_handle,\n\u001b[32m    276\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    279\u001b[39m         **kwargs,\n\u001b[32m    280\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:140\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    130\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    132\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    144\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\marti\\\\OneDrive\\\\Documentos\\\\Martim\\\\Académico\\\\Master LBS\\\\Clases MAM2025\\\\LondonLAB\\\\data\\\\processed\\\\slot_level.parquet'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_slot = pd.read_parquet(SLOT_PARQUET)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Still cannot find the file at the constructed path: {SLOT_PARQUET}\")\n",
    "    print(\"Please double-check that the file exists and the path logic is correct for your environment.\")\n",
    "    raise # Re-raise the exception if it still occurs\n",
    "\n",
    "print(f\"Loaded data from {SLOT_PARQUET}. Shape: {df_slot.shape}\")\n",
    "df_slot.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2] or equivalent in 2.py, after loading df_slot\n",
    "df_slot = pd.read_parquet(SLOT_PARQUET)\n",
    "print(f\"Loaded data from {SLOT_PARQUET}. Shape: {df_slot.shape}\")\n",
    "print(\"Unique Venues in df_slot:\", df_slot['Venue Name'].unique()) # <--- ADD THIS\n",
    "if 'Birmingham' not in df_slot['Venue Name'].unique():\n",
    "    print(\"WARNING: Birmingham is NOT present in the loaded slot_level.parquet.\")\n",
    "    print(\"This likely means it was filtered out in 1_ingest.ipynb, probably by the 'Search At >= 2024-08-01' filter.\")\n",
    "    print(\"Please check your raw data and the filters in 1_ingest.ipynb.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that it has all needed columns\n",
    "required_cols = ['Venue Name', 'search_date_for', 'search_hour_for', \n",
    "                 'n_searches', 'n_bookings', 'booking_rate', \n",
    "                 'lead_time', 'day_of_week', 'is_weekend', \n",
    "                 'avg_price', 'pct_avail', 'capacity', \n",
    "                 'min_price', 'max_price']\n",
    "\n",
    "missing_cols = [col for col in required_cols if col not in df_slot.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "else:\n",
    "    print(\"All required columns are present in df_slot.\")\n",
    "\n",
    "if df_slot.empty:\n",
    "    raise ValueError(\"df_slot is empty. This might be due to filters in the previous notebook (e.g., 'Search At >= 2024-08-01'). Please check data processing steps.\")\n",
    "\n",
    "# Handle potential NaNs that could affect clustering/modeling\n",
    "# For clustering, we'll select specific columns and handle NaNs there.\n",
    "# For modeling, NaNs in features like lead_time, avg_price should be imputed.\n",
    "df_slot['lead_time'] = df_slot['lead_time'].fillna(df_slot['lead_time'].median())\n",
    "df_slot['avg_price'] = df_slot['avg_price'].fillna(df_slot['avg_price'].median())\n",
    "# pct_avail might also need imputation if used directly\n",
    "df_slot['pct_avail'] = df_slot['pct_avail'].fillna(df_slot['pct_avail'].mean())\n",
    "\n",
    "\n",
    "# Ensure booking_rate is not NaN or Inf, cap at 1 for safety if n_bookings > n_searches (unlikely from agg)\n",
    "df_slot['booking_rate'] = (df_slot['n_bookings'] / df_slot['n_searches']).fillna(0)\n",
    "df_slot.loc[df_slot['n_searches'] == 0, 'booking_rate'] = 0 # Ensure 0 if no searches\n",
    "df_slot['booking_rate'] = np.clip(df_slot['booking_rate'], 0, 1) # Ensure it's a probability\n",
    "\n",
    "print(\"\\nData after initial checks and NaN handling for key modeling features:\")\n",
    "df_slot[required_cols].info()\n",
    "df_slot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Slot-Level Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Feature selection for clustering\n",
    "# At minimum: booking_rate; optionally add lead_time or avg_price.\n",
    "# We'll use booking_rate, lead_time, and avg_price.\n",
    "# Note: avg_price can vary significantly, so scaling is important.\n",
    "cluster_features = ['booking_rate', 'lead_time', 'avg_price']\n",
    "df_cluster_data = df_slot[cluster_features + ['Venue Name']].copy()\n",
    "\n",
    "# Impute NaNs for clustering features if any remain (should be handled above, but as a safeguard)\n",
    "for col in cluster_features:\n",
    "    if df_cluster_data[col].isnull().any():\n",
    "        df_cluster_data[col] = df_cluster_data[col].fillna(df_cluster_data[col].median())\n",
    "\n",
    "print(f\"\\nFeatures for clustering: {cluster_features}\")\n",
    "df_cluster_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Elbow + silhouette (Example for one venue, then per-venue auto-K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll do this within the per-venue loop.\n",
    "\n",
    "def plot_elbow_silhouette(X_scaled, max_k=10, title_prefix=\"\"):\n",
    "    inertia = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    k_range_inertia = range(1, max_k + 1)\n",
    "    k_range_silhouette = range(2, max_k + 1)\n",
    "\n",
    "    for k in k_range_inertia:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "\n",
    "    for k in k_range_silhouette:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        if len(np.unique(labels)) > 1: # Silhouette score is only defined if there is more than 1 cluster\n",
    "             silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
    "        else: # if only one cluster predicted (e.g. k=1 or all points are same)\n",
    "            silhouette_scores.append(-1) # Invalid score placeholder\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Number of Clusters (k)')\n",
    "    ax1.set_ylabel('Inertia', color=color)\n",
    "    ax1.plot(k_range_inertia, inertia, marker='o', color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set_title(f'{title_prefix}Elbow Method & Silhouette Score')\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Silhouette Score', color=color)\n",
    "    # Align silhouette plot: silhouette_scores has k-2 elements, k_range_silhouette has k-1.\n",
    "    ax2.plot(k_range_silhouette, silhouette_scores, marker='x', linestyle='--', color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "    \n",
    "    # Determine best k based on silhouette (simple max)\n",
    "    if silhouette_scores:\n",
    "        best_k_silhouette = k_range_silhouette[np.argmax(silhouette_scores)]\n",
    "        print(f\"{title_prefix}Suggested k based on max silhouette score: {best_k_silhouette} (score: {max(silhouette_scores):.2f})\")\n",
    "        return best_k_silhouette\n",
    "    return 3 # Default k if silhouette fails\n",
    "\n",
    "print(\"Elbow/Silhouette plotting function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Per-venue auto-K and 4. Label semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slot_clustered = df_slot.copy()\n",
    "df_slot_clustered['km_cluster'] = -1 # Initialize cluster column\n",
    "df_slot_clustered['km_cluster_label'] = 'unknown'\n",
    "\n",
    "all_venue_clusters_info = [] # To store (venue, date, hour, slot_label, km_cluster)\n",
    "\n",
    "for venue in df_cluster_data['Venue Name'].unique():\n",
    "    print(f\"\\n--- Clustering for Venue: {venue} ---\")\n",
    "    venue_data = df_cluster_data[df_cluster_data['Venue Name'] == venue][cluster_features]\n",
    "    \n",
    "    if venue_data.shape[0] < 10: # Not enough data points for meaningful clustering\n",
    "        print(f\"Skipping {venue} due to insufficient data points ({venue_data.shape[0]})\")\n",
    "        # Assign a default cluster or handle as needed\n",
    "        df_slot_clustered.loc[df_slot_clustered['Venue Name'] == venue, 'km_cluster'] = 0\n",
    "        df_slot_clustered.loc[df_slot_clustered['Venue Name'] == venue, 'km_cluster_label'] = 'default_low_data'\n",
    "        \n",
    "        # Add to all_venue_clusters_info\n",
    "        venue_slot_indices = df_slot_clustered[df_slot_clustered['Venue Name'] == venue].index\n",
    "        temp_df = df_slot_clustered.loc[venue_slot_indices, ['Venue Name', 'search_date_for', 'search_hour_for', 'km_cluster_label', 'km_cluster']]\n",
    "        all_venue_clusters_info.append(temp_df)\n",
    "        continue\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(venue_data)\n",
    "\n",
    "    # Determine best k for this venue\n",
    "    # Set max_k dynamically, e.g., min(10, num_samples-1), but ensure it's at least 2 for silhouette\n",
    "    max_k_venue = min(10, X_scaled.shape[0]-1 if X_scaled.shape[0] > 1 else 1)\n",
    "    if max_k_venue < 2 : # Silhouette score requires at least 2 clusters\n",
    "        print(f\"Not enough samples in {venue} for varied k, assigning to single cluster.\")\n",
    "        best_k = 1\n",
    "        labels = np.zeros(X_scaled.shape[0], dtype=int) # All in one cluster\n",
    "    else:\n",
    "        best_k = plot_elbow_silhouette(X_scaled, max_k=max_k_venue, title_prefix=f\"{venue}: \")\n",
    "        if best_k < 2 : best_k = 2 # Ensure at least 2 clusters if silhouette suggested 1 from bad scores\n",
    "        if best_k > X_scaled.shape[0]-1 : best_k = X_scaled.shape[0]-1 # cap k\n",
    "\n",
    "        kmeans = KMeans(n_clusters=best_k, random_state=42, n_init='auto')\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    # Store numeric cluster labels for this venue\n",
    "    venue_slot_indices = df_slot_clustered[df_slot_clustered['Venue Name'] == venue].index\n",
    "    df_slot_clustered.loc[venue_slot_indices, 'km_cluster'] = labels\n",
    "    \n",
    "    # 4. Label semantics: Map numeric clusters to ordered labels based on booking_rate\n",
    "    temp_venue_df = df_slot_clustered.loc[venue_slot_indices].copy()\n",
    "    temp_venue_df['numeric_cluster_temp'] = labels # Use the fresh labels\n",
    "    \n",
    "    if len(np.unique(labels)) > 1:\n",
    "        cluster_means = temp_venue_df.groupby('numeric_cluster_temp')['booking_rate'].mean().sort_values()\n",
    "        \n",
    "        # Define peak labels dynamically based on number of clusters\n",
    "        if len(cluster_means) == 1:\n",
    "            peak_labels = ['standard_peak']\n",
    "        elif len(cluster_means) == 2:\n",
    "            peak_labels = ['off_peak', 'peak']\n",
    "        elif len(cluster_means) == 3:\n",
    "            peak_labels = ['off_peak', 'mid_peak', 'super_peak']\n",
    "        else: # For > 3 clusters, use a generic naming scheme\n",
    "            peak_labels = [f'peak_level_{i}' for i in range(len(cluster_means))]\n",
    "            \n",
    "        # Create mapping from sorted numeric cluster to semantic label\n",
    "        label_mapping = {cluster_idx: peak_labels[i] for i, cluster_idx in enumerate(cluster_means.index)}\n",
    "        \n",
    "        df_slot_clustered.loc[venue_slot_indices, 'km_cluster_label'] = temp_venue_df['numeric_cluster_temp'].map(label_mapping)\n",
    "    else: # Single cluster case\n",
    "         df_slot_clustered.loc[venue_slot_indices, 'km_cluster_label'] = 'standard_peak'\n",
    "\n",
    "\n",
    "    # Add to all_venue_clusters_info for CSV export\n",
    "    temp_df_export = df_slot_clustered.loc[venue_slot_indices, ['Venue Name', 'search_date_for', 'search_hour_for', 'km_cluster_label', 'km_cluster']]\n",
    "    all_venue_clusters_info.append(temp_df_export)\n",
    "\n",
    "print(\"\\nPer-venue clustering and label assignment complete.\")\n",
    "df_slot_clustered[['Venue Name', 'km_cluster', 'km_cluster_label', 'booking_rate']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save time_slot_clusters.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_venue_clusters_info:\n",
    "    df_time_slot_clusters = pd.concat(all_venue_clusters_info)\n",
    "    df_time_slot_clusters.to_csv(CLUSTER_CSV, index=False)\n",
    "    print(f\"\\nSaved time slot cluster information to {CLUSTER_CSV}\")\n",
    "else:\n",
    "    print(\"\\nNo cluster information to save (all_venue_clusters_info is empty).\")\n",
    "\n",
    "# Display some cluster stats\n",
    "print(\"\\nCluster distribution by venue:\")\n",
    "print(df_slot_clustered.groupby(['Venue Name', 'km_cluster_label'])['booking_rate'].agg(['mean', 'count']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Demand-Prediction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# Target: n_bookings (count)\n",
    "# Features: avg_price, search_hour_for, km_cluster_label (categorical), lead_time, day_of_week, is_weekend, Venue Name, capacity\n",
    "# km_cluster_label is preferred over km_cluster (numeric) as it has semantic meaning and avoids issues with numeric interpretation.\n",
    "\n",
    "df_model = df_slot_clustered.copy()\n",
    "\n",
    "# Ensure target is not NaN and is integer\n",
    "df_model['n_bookings'] = df_model['n_bookings'].fillna(0).astype(int)\n",
    "\n",
    "# Define features and target\n",
    "TARGET = 'n_bookings'\n",
    "\n",
    "# Numerical features\n",
    "numerical_features = ['avg_price', 'lead_time', 'capacity'] # search_hour_for, day_of_week, is_weekend can be treated as categorical or numerical\n",
    "# Categorical features\n",
    "categorical_features = ['Venue Name', 'km_cluster_label', 'search_hour_for', 'day_of_week', 'is_weekend']\n",
    "\n",
    "\n",
    "# Handle missing values in features selected for modeling (should be minimal after earlier steps)\n",
    "for col in numerical_features:\n",
    "    df_model[col] = df_model[col].fillna(df_model[col].median())\n",
    "for col in categorical_features:\n",
    "    df_model[col] = df_model[col].fillna(df_model[col].mode()[0])\n",
    "    df_model[col] = df_model[col].astype('category') # Ensure they are category type for CatBoost/LGBM\n",
    "\n",
    "\n",
    "X = df_model[numerical_features + categorical_features]\n",
    "y = df_model[TARGET]\n",
    "\n",
    "# Split data (chronological split is often best for time-series related data, but problem asks for rolling window in Optuna)\n",
    "# For initial baselines, a simple random split. For Optuna, TimeSeriesSplit.\n",
    "# Using search_date_for to sort for potential time-series split\n",
    "df_model_sorted = df_model.sort_values(by=['Venue Name', 'search_date_for', 'search_hour_for'])\n",
    "X_sorted = df_model_sorted[numerical_features + categorical_features]\n",
    "y_sorted = df_model_sorted[TARGET]\n",
    "\n",
    "# For baselines, let's use a standard train-test split for simplicity of comparison here.\n",
    "# For hyperparameter tuning and final model, TimeSeriesSplit will be used.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nData prepared for modeling. X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... later, after creating X_test ... \n",
    "# In[8] or equivalent, after X_train, X_test, y_train, y_test = train_test_split(X, y, ...)\n",
    "print(\"Unique Venues in X_test:\", X_test['Venue Name'].unique()) # <--- ADD THIS\n",
    "if 'Birmingham' not in X_test['Venue Name'].unique() and 'Birmingham' in df_model['Venue Name'].unique():\n",
    "    print(\"WARNING: Birmingham was in df_model but not in X_test. This might be due to the train-test split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn's PoissonRegressor and LogisticRegression (for n_bookings > 0)\n",
    "\n",
    "# For Logistic Regression, target needs to be binary\n",
    "y_train_binary = (y_train > 0).astype(int)\n",
    "y_test_binary = (y_test > 0).astype(int)\n",
    "\n",
    "# Preprocessing for linear models: Scale numerical, OneHotEncode categorical\n",
    "preprocessor_linear = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_features) # Ordinal for simplicity here, OHE might be better\n",
    "    ],\n",
    "    remainder='passthrough' # Keep any other columns if they exist\n",
    ")\n",
    "\n",
    "# --- Logistic Regression (Baseline for P(booking > 0)) ---\n",
    "pipeline_logreg = Pipeline(steps=[('preprocessor', preprocessor_linear),\n",
    "                                  ('classifier', LogisticRegression(random_state=42, solver='liblinear', max_iter=1000))])\n",
    "pipeline_logreg.fit(X_train, y_train_binary)\n",
    "y_pred_logreg_proba = pipeline_logreg.predict_proba(X_test)[:, 1]\n",
    "auc_logreg = roc_auc_score(y_test_binary, y_pred_logreg_proba)\n",
    "print(f\"Baseline Logistic Regression AUC (for P(n_bookings > 0)): {auc_logreg:.4f}\")\n",
    "\n",
    "# --- Poisson Regression (Baseline for n_bookings count) ---\n",
    "# PoissonRegressor doesn't like negative inputs from StandardScaler if features can be zero\n",
    "# Using a slightly different preprocessor for Poisson or ensuring non-negativity\n",
    "# For simplicity, let's fit Poisson on data that's not aggressively scaled below zero or use original scales.\n",
    "# Or, ensure 'capacity' and 'lead_time' are non-negative, 'avg_price' can be tricky if standardized.\n",
    "# Let's use OrdinalEncoder for categoricals and keep numerical as is or use MinMaxScaler.\n",
    "preprocessor_poisson = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(with_mean=False), numerical_features), # Scale but don't center, or use MinMaxScaler\n",
    "        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, \n",
    "                               min_frequency=5), # min_frequency to handle rare categories\n",
    "                               categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipeline_poisson = Pipeline(steps=[('preprocessor', preprocessor_poisson),\n",
    "                                   ('regressor', PoissonRegressor(alpha=1.0, max_iter=1000))]) # alpha is L2 penalty\n",
    "\n",
    "try:\n",
    "    X_train_poisson = preprocessor_poisson.fit_transform(X_train)\n",
    "    X_test_poisson = preprocessor_poisson.transform(X_test)\n",
    "    \n",
    "    # PoissonRegressor requires non-negative X if using certain solvers or no intercept.\n",
    "    # The default 'lbfgs' solver should be fine.\n",
    "    # If X_train_poisson has negative values after scaling and it causes issues:\n",
    "    # X_train_poisson[X_train_poisson < 0] = 0 # Simplistic clamp, better to use MinMaxScaler\n",
    "    \n",
    "    pipeline_poisson.fit(X_train, y_train) # Fit on original X_train, pipeline handles preprocessing\n",
    "    y_pred_poisson = pipeline_poisson.predict(X_test)\n",
    "    \n",
    "    rmse_poisson = np.sqrt(mean_squared_error(y_test, y_pred_poisson))\n",
    "    mae_poisson = mean_absolute_error(y_test, y_pred_poisson)\n",
    "    # Deviance for Poisson: 2 * (sum(y_true * log(y_true/y_pred)) - sum(y_true - y_pred))\n",
    "    # Sklearn's PoissonRegressor has a score method which is D^2 (explained deviance), not raw deviance.\n",
    "    # For simplicity, RMSE and MAE are good indicators for count models.\n",
    "    print(f\"Baseline Poisson Regression RMSE: {rmse_poisson:.4f}, MAE: {mae_poisson:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in Poisson Regressor: {e}. This might be due to negative values in preprocessed X for Poisson.\")\n",
    "    rmse_poisson, mae_poisson = np.nan, np.nan\n",
    "\n",
    "\n",
    "baseline_metrics = {\n",
    "    'LogisticRegression_AUC': auc_logreg,\n",
    "    'PoissonRegressor_RMSE': rmse_poisson,\n",
    "    'PoissonRegressor_MAE': mae_poisson,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gradient-Boosted Trees: LightGBM & CatBoost (Default Params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target: n_bookings (count) -> Regression task for GBMs\n",
    "\n",
    "# --- LightGBM ---\n",
    "# LGBM can handle categorical features directly if specified\n",
    "# Convert categorical feature names to list of strings for LGBM\n",
    "lgbm_categorical_features = list(X_train.select_dtypes(include='category').columns)\n",
    "\n",
    "lgbm_model_default = lgb.LGBMRegressor(random_state=42)\n",
    "lgbm_model_default.fit(X_train, y_train, categorical_feature=lgbm_categorical_features)\n",
    "y_pred_lgbm_default = lgbm_model_default.predict(X_test)\n",
    "rmse_lgbm_default = np.sqrt(mean_squared_error(y_test, y_pred_lgbm_default))\n",
    "mae_lgbm_default = mean_absolute_error(y_test, y_pred_lgbm_default)\n",
    "print(f\"LightGBM (Default) RMSE: {rmse_lgbm_default:.4f}, MAE: {mae_lgbm_default:.4f}\")\n",
    "\n",
    "\n",
    "# --- CatBoost ---\n",
    "# CatBoost handles categorical features natively\n",
    "cat_features_indices = [X_train.columns.get_loc(col) for col in categorical_features]\n",
    "\n",
    "cb_model_default = cb.CatBoostRegressor(random_state=42, verbose=0, cat_features=cat_features_indices)\n",
    "cb_model_default.fit(X_train, y_train)\n",
    "y_pred_cb_default = cb_model_default.predict(X_test)\n",
    "rmse_cb_default = np.sqrt(mean_squared_error(y_test, y_pred_cb_default))\n",
    "mae_cb_default = mean_absolute_error(y_test, y_pred_cb_default)\n",
    "print(f\"CatBoost (Default) RMSE: {rmse_cb_default:.4f}, MAE: {mae_cb_default:.4f}\")\n",
    "\n",
    "default_gbm_metrics = {\n",
    "    'LGBM_Default_RMSE': rmse_lgbm_default,\n",
    "    'LGBM_Default_MAE': mae_lgbm_default,\n",
    "    'CatBoost_Default_RMSE': rmse_cb_default,\n",
    "    'CatBoost_Default_MAE': mae_cb_default,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter Tuning (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TimeSeriesSplit for cross-validation to respect temporal order if applicable\n",
    "# X_sorted and y_sorted are used here.\n",
    "\n",
    "# Make sure categorical features in X_sorted are of 'category' dtype for LGBM/CatBoost\n",
    "for col in categorical_features:\n",
    "    X_sorted[col] = X_sorted[col].astype('category')\n",
    "\n",
    "# Time series split\n",
    "tscv = TimeSeriesSplit(n_splits=3) # Small number of splits for demonstration\n",
    "\n",
    "# --- Optuna for LightGBM ---\n",
    "def objective_lgbm(trial):\n",
    "    # Convert categorical features to codes for this Optuna run if not handled by LGBM directly with 'category' dtype\n",
    "    # X_train_lgbm, X_val_lgbm = X_sorted.iloc[train_index], X_sorted.iloc[val_index]\n",
    "    # y_train_lgbm, y_val_lgbm = y_sorted.iloc[train_index], y_sorted.iloc[val_index]\n",
    "    # Ensure categoricals are handled\n",
    "    # for col in categorical_features:\n",
    "    #     X_train_lgbm[col] = X_train_lgbm[col].cat.codes\n",
    "    #     X_val_lgbm[col] = X_val_lgbm[col].cat.codes\n",
    "        \n",
    "    params = {\n",
    "        'objective': 'poisson', # Good for count data\n",
    "        'metric': 'rmse',\n",
    "        'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0), # L1\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0), # L2\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    \n",
    "    # Cross-validation using TimeSeriesSplit\n",
    "    # Optuna integrates with LGBM CV, but manual loop for clarity with TSCV\n",
    "    rmses = []\n",
    "    for train_index, val_index in tscv.split(X_sorted):\n",
    "        X_train_fold, X_val_fold = X_sorted.iloc[train_index], X_sorted.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_sorted.iloc[train_index], y_sorted.iloc[val_index]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold, \n",
    "                  eval_set=[(X_val_fold, y_val_fold)],\n",
    "                  eval_metric='rmse',\n",
    "                  #callbacks=[lgb.early_stopping(50, verbose=False)], # Corrected: early_stopping is a callback\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=-1)],\n",
    "                  categorical_feature=categorical_features # Pass as list of names\n",
    "                 )\n",
    "        preds = model.predict(X_val_fold)\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_val_fold, preds)))\n",
    "    \n",
    "    return np.mean(rmses)\n",
    "\n",
    "print(\"\\nStarting Optuna for LightGBM...\")\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=20) # Reduced trials for speed; increase for better results (e.g., 50-100)\n",
    "best_params_lgbm = study_lgbm.best_params\n",
    "print(\"Best LightGBM params:\", best_params_lgbm)\n",
    "\n",
    "lgbm_tuned = lgb.LGBMRegressor(objective='poisson', metric='rmse', random_state=42, **best_params_lgbm)\n",
    "lgbm_tuned.fit(X_train, y_train, categorical_feature=categorical_features) # Final fit on X_train, y_train (non-sorted for consistency with test set)\n",
    "y_pred_lgbm_tuned = lgbm_tuned.predict(X_test)\n",
    "rmse_lgbm_tuned = np.sqrt(mean_squared_error(y_test, y_pred_lgbm_tuned))\n",
    "mae_lgbm_tuned = mean_absolute_error(y_test, y_pred_lgbm_tuned)\n",
    "print(f\"LightGBM (Tuned) RMSE: {rmse_lgbm_tuned:.4f}, MAE: {mae_lgbm_tuned:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Optuna for CatBoost ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost needs categorical feature indices\n",
    "cat_features_indices_sorted = [X_sorted.columns.get_loc(col) for col in categorical_features]\n",
    "\n",
    "def objective_cb(trial):\n",
    "    params = {\n",
    "        'objective': 'RMSE', # CatBoost uses 'RMSE' for regression, Poisson is also an option: 'Poisson'\n",
    "        'random_seed': 42,\n",
    "        'verbose': 0,\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000, step=100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255), # For numerical features\n",
    "         'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0), # Similar to colsample_bytree\n",
    "    }\n",
    "    \n",
    "    model = cb.CatBoostRegressor(**params)\n",
    "    rmses = []\n",
    "    for train_index, val_index in tscv.split(X_sorted):\n",
    "        X_train_fold, X_val_fold = X_sorted.iloc[train_index], X_sorted.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_sorted.iloc[train_index], y_sorted.iloc[val_index]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold, \n",
    "                  eval_set=[(X_val_fold, y_val_fold)],\n",
    "                  cat_features=cat_features_indices_sorted, \n",
    "                  early_stopping_rounds=50, verbose=0)\n",
    "        preds = model.predict(X_val_fold)\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_val_fold, preds)))\n",
    "        \n",
    "    return np.mean(rmses)\n",
    "\n",
    "print(\"\\nStarting Optuna for CatBoost...\")\n",
    "study_cb = optuna.create_study(direction='minimize')\n",
    "study_cb.optimize(objective_cb, n_trials=20) # Reduced trials for speed\n",
    "best_params_cb = study_cb.best_params\n",
    "print(\"Best CatBoost params:\", best_params_cb)\n",
    "\n",
    "cb_tuned = cb.CatBoostRegressor(objective='RMSE', random_seed=42, verbose=0, **best_params_cb)\n",
    "cb_tuned.fit(X_train, y_train, cat_features=cat_features_indices) # Final fit on X_train, y_train\n",
    "y_pred_cb_tuned = cb_tuned.predict(X_test)\n",
    "rmse_cb_tuned = np.sqrt(mean_squared_error(y_test, y_pred_cb_tuned))\n",
    "mae_cb_tuned = mean_absolute_error(y_test, y_pred_cb_tuned)\n",
    "print(f\"CatBoost (Tuned) RMSE: {rmse_cb_tuned:.4f}, MAE: {mae_cb_tuned:.4f}\")\n",
    "\n",
    "\n",
    "tuned_gbm_metrics = {\n",
    "    'LGBM_Tuned_RMSE': rmse_lgbm_tuned,\n",
    "    'LGBM_Tuned_MAE': mae_lgbm_tuned,\n",
    "    'CatBoost_Tuned_RMSE': rmse_cb_tuned,\n",
    "    'CatBoost_Tuned_MAE': mae_cb_tuned,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Calibration & Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calibration (for regressors, this is about reliability of point estimates) ---\n",
    "# Plot predicted vs. actual (binned)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_cb_tuned, alpha=0.3, label='CatBoost Tuned Predictions')\n",
    "plt.scatter(y_test, y_pred_lgbm_tuned, alpha=0.3, label='LightGBM Tuned Predictions', marker='x')\n",
    "plt.plot([min(y_test.min(),0), y_test.max()], [min(y_test.min(),0), y_test.max()], '--', color='red', label='Perfect Prediction')\n",
    "plt.xlabel(\"Actual n_bookings\")\n",
    "plt.ylabel(\"Predicted n_bookings\")\n",
    "plt.title(\"Predicted vs. Actual n_bookings (Tuned Models)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(MODELS_DIR,\"predicted_vs_actual.png\"))\n",
    "plt.show()\n",
    "print(\"Predicted vs Actual plot saved.\")\n",
    "\n",
    "# Reliability curve is for classifiers. For regressors, we look at residual plots or binned pred vs actual.\n",
    "\n",
    "# --- SHAP ---\n",
    "# Using the (potentially) best model, CatBoost Tuned, for SHAP.\n",
    "# SHAP works with models that output raw scores.\n",
    "# Need to ensure X_test has numerical categories if CatBoost was trained with integer codes for categories internally.\n",
    "# However, CatBoost's SHAP implementation usually handles its Pool object well.\n",
    "\n",
    "# Create a SHAP explainer for CatBoost\n",
    "# For CatBoost, it's often better to use its internal SHAP value calculation method if available,\n",
    "# or pass data in the format it expects (e.g., with Pool).\n",
    "# The `shap.TreeExplainer` works well with CatBoost.\n",
    "explainer_cb = shap.TreeExplainer(cb_tuned)\n",
    "shap_values_cb = explainer_cb.shap_values(X_test) # Use the same X_test format as for prediction\n",
    "\n",
    "plt.figure() # Create a new figure to avoid interference\n",
    "shap.summary_plot(shap_values_cb, X_test, show=False)\n",
    "plt.title(\"SHAP Summary Plot (CatBoost Tuned)\")\n",
    "plt.savefig(os.path.join(MODELS_DIR, \"shap_summary.png\"), bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"SHAP summary plot saved to shap_summary.png\")\n",
    "\n",
    "\n",
    "# --- Feature Importance ---\n",
    "# For CatBoost\n",
    "feature_importances_cb = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance_cb': cb_tuned.get_feature_importance()\n",
    "}).sort_values(by='importance_cb', ascending=False)\n",
    "\n",
    "# For LightGBM\n",
    "feature_importances_lgbm = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance_lgbm': lgbm_tuned.feature_importances_\n",
    "}).sort_values(by='importance_lgbm', ascending=False)\n",
    "\n",
    "# Combine and save\n",
    "feature_importances_combined = pd.merge(feature_importances_lgbm, feature_importances_cb, on='feature', how='outer')\n",
    "feature_importances_combined = feature_importances_combined.fillna(0) # if a feature was 0 importance in one model\n",
    "feature_importances_combined.to_csv(os.path.join(MODELS_DIR, \"feature_importances.csv\"), index=False)\n",
    "print(\"\\nFeature importances (Top 10 CatBoost):\")\n",
    "print(feature_importances_combined.sort_values(by='importance_cb', ascending=False).head(10))\n",
    "print(f\"Feature importances saved to {os.path.join(MODELS_DIR, 'feature_importances.csv')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Selection & Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models (RMSE, MAE)\n",
    "# Lower is better for RMSE and MAE\n",
    "print(\"\\n--- Model Performance Summary ---\")\n",
    "print(\"Baseline Metrics:\", baseline_metrics)\n",
    "print(\"Default GBM Metrics:\", default_gbm_metrics)\n",
    "print(\"Tuned GBM Metrics:\", tuned_gbm_metrics)\n",
    "\n",
    "# Choose final model (e.g., CatBoost Tuned if it performs best)\n",
    "# For this example, let's assume CatBoost Tuned is the best.\n",
    "best_model = cb_tuned\n",
    "best_model_name = \"catboost_tuned\"\n",
    "chosen_model_path = os.path.join(MODELS_DIR, \"best_model.pkl\")\n",
    "\n",
    "# Criteria for selection:\n",
    "# 1. RMSE/MAE (lower is better)\n",
    "# 2. Calibration (visual inspection of pred vs. actual)\n",
    "# 3. Inference latency (not measured here, but CatBoost can be slower than LGBM)\n",
    "# 4. Robustness, ease of use.\n",
    "\n",
    "# Example: choose based on Tuned CatBoost RMSE vs Tuned LGBM RMSE\n",
    "if tuned_gbm_metrics['CatBoost_Tuned_RMSE'] < tuned_gbm_metrics['LGBM_Tuned_RMSE']:\n",
    "    best_model = cb_tuned\n",
    "    best_model_name = \"catboost_tuned\"\n",
    "    print(f\"\\nSelected CatBoost (Tuned) as the best model with RMSE: {tuned_gbm_metrics['CatBoost_Tuned_RMSE']:.4f}\")\n",
    "else:\n",
    "    best_model = lgbm_tuned\n",
    "    best_model_name = \"lightgbm_tuned\"\n",
    "    print(f\"\\nSelected LightGBM (Tuned) as the best model with RMSE: {tuned_gbm_metrics['LGBM_Tuned_RMSE']:.4f}\")\n",
    "\n",
    "\n",
    "joblib.dump(best_model, chosen_model_path)\n",
    "print(f\"Saved best model ({best_model_name}) to {chosen_model_path}\")\n",
    "\n",
    "# Optionally save the other tuned GBM as backup\n",
    "if best_model_name == \"catboost_tuned\":\n",
    "    backup_model = lgbm_tuned\n",
    "    backup_model_name = \"lightgbm_tuned\"\n",
    "else:\n",
    "    backup_model = cb_tuned\n",
    "    backup_model_name = \"catboost_tuned\"\n",
    "\n",
    "backup_model_path = os.path.join(MODELS_DIR, f\"{backup_model_name}_backup.pkl\")\n",
    "joblib.dump(backup_model, backup_model_path)\n",
    "print(f\"Saved backup model ({backup_model_name}) to {backup_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Price-Response Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Grid definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define price range: use min_price/max_price from slot_level, or a dynamic range.\n",
    "# The `slot_level` data has `min_price` and `max_price` columns, which are fallback bounds.\n",
    "# `avg_price` is the actual price shown for that slot observation.\n",
    "# For the grid, we want to vary the `avg_price` feature.\n",
    "\n",
    "# Let's define a general price range. Example: from £5 to £50 with £1 step.\n",
    "# A more dynamic way would be venue-specific, using its historical min/max observed prices.\n",
    "min_sim_price = df_model['avg_price'].min() if not df_model.empty else 5.0\n",
    "max_sim_price = df_model['avg_price'].max() if not df_model.empty else 50.0\n",
    "# Ensure min_sim_price is not NaN and is reasonable\n",
    "min_sim_price = max(5.0, min_sim_price if pd.notna(min_sim_price) else 5.0)\n",
    "max_sim_price = min(100.0, max_sim_price if pd.notna(max_sim_price) else 50.0) # Cap at 100 for sanity\n",
    "if min_sim_price >= max_sim_price: max_sim_price = min_sim_price + 10 # Ensure range\n",
    "\n",
    "price_step = 1.0\n",
    "simulation_prices = np.arange(min_sim_price, max_sim_price + price_step, price_step)\n",
    "print(f\"\\nPrice range for simulation: £{min_sim_price} to £{max_sim_price} with step £{price_step}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Expected revenue calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each (venue, date, hour) representative slot and each price:\n",
    "#   predicted_n_bookings = model.predict(featurized_row_with_price)\n",
    "#   exp_rev = predicted_n_bookings * price\n",
    "\n",
    "# We need representative slots. Let's pick unique combinations of features OTHER than price.\n",
    "# Use `df_model` (which contains `X` and `y` before split) as the source of representative slots.\n",
    "# Keep only one row for each combination of key slot characteristics.\n",
    "# Key characteristics: Venue Name, search_date_for, search_hour_for, km_cluster_label, day_of_week, is_weekend, capacity\n",
    "# We will iterate through these unique slots.\n",
    "\n",
    "# Take a sample of unique slots to avoid huge computation if many unique slots.\n",
    "# The `search_date_for` makes slots very unique. Let's try to find representative non-date specific slots.\n",
    "# Or, average features for typical slot types.\n",
    "# For simplicity, let's use the test set slots (X_test) and vary their 'avg_price'.\n",
    "# X_test is already a sample.\n",
    "\n",
    "price_response_data = []\n",
    "\n",
    "# Ensure X_test_copy has categorical features as 'category' dtype if model expects it\n",
    "# Best_model (CatBoost/LGBM) can handle 'category' dtype if trained that way.\n",
    "# X_test used for prediction has original dtypes, which should be fine if model was trained on similar X_train.\n",
    "\n",
    "print(f\"Generating price response grid using {X_test.shape[0]} sample slots from test set...\")\n",
    "counter = 0\n",
    "for idx, slot_features_orig in X_test.iterrows():\n",
    "    counter +=1\n",
    "    if counter % 100 == 0 : print(f\"Processing slot {counter}/{X_test.shape[0]}\")\n",
    "    \n",
    "    slot_features = slot_features_orig.copy() # Base features for this slot\n",
    "    \n",
    "    # Original slot characteristics (for grouping/identification)\n",
    "    venue = slot_features['Venue Name']\n",
    "    # date = slot_features['search_date_for'] # If 'search_date_for' was in X_test\n",
    "    # hour = slot_features['search_hour_for'] # If 'search_hour_for' was in X_test\n",
    "    # These might not be in X_test if they were handled differently (e.g. part of df_model index)\n",
    "    # Let's assume 'search_date_for' is not a direct feature but can be fetched via index if needed.\n",
    "    # For this example, we'll just use index from X_test to identify the slot.\n",
    "    \n",
    "    original_date = df_model.loc[idx, 'search_date_for']\n",
    "    original_hour = df_model.loc[idx, 'search_hour_for']\n",
    "\n",
    "    for price in simulation_prices:\n",
    "        slot_features['avg_price'] = price # Set the new price\n",
    "        \n",
    "        # Ensure feature order and types match what the model expects (DataFrame input)\n",
    "        feature_row_df = pd.DataFrame([slot_features])\n",
    "        \n",
    "        # Predict n_bookings\n",
    "        # The model 'best_model' expects categorical features to be of dtype 'category' if trained that way.\n",
    "        # X_train had them converted. Ensure feature_row_df also has them.\n",
    "        for cat_col in categorical_features:\n",
    "            if cat_col in feature_row_df.columns:\n",
    "                 # Ensure it's category type with known categories from training data\n",
    "                 # This is tricky. Simpler: ensure model's fit on X_train (which had category dtypes) makes it robust.\n",
    "                 # CatBoost generally handles this well if cat_features indices are provided.\n",
    "                 # LGBM needs categorical_feature='auto' or specific list of names.\n",
    "                 # If X_test (and thus slot_features_orig) has them as category, it should be fine.\n",
    "                 feature_row_df[cat_col] = feature_row_df[cat_col].astype(X_train[cat_col].dtype)\n",
    "\n",
    "\n",
    "        predicted_n_bookings = best_model.predict(feature_row_df)[0]\n",
    "        predicted_n_bookings = max(0, predicted_n_bookings) # Ensure non-negative bookings\n",
    "\n",
    "        expected_revenue = predicted_n_bookings * price\n",
    "        \n",
    "        price_response_data.append({\n",
    "            'slot_id_test_set_idx': idx, # To identify the base slot from X_test\n",
    "            'Venue Name': venue,\n",
    "            'search_date_for': original_date, # Original date of the slot from X_test\n",
    "            'search_hour_for': original_hour, # Original hour\n",
    "            'km_cluster_label': slot_features['km_cluster_label'],\n",
    "            'day_of_week': slot_features['day_of_week'],\n",
    "            'is_weekend': slot_features['is_weekend'],\n",
    "            'capacity': slot_features['capacity'],\n",
    "            'simulated_price': price,\n",
    "            'predicted_n_bookings': predicted_n_bookings,\n",
    "            'expected_revenue': expected_revenue\n",
    "        })\n",
    "\n",
    "df_price_response = pd.DataFrame(price_response_data)\n",
    "print(f\"\\nGenerated price response data with {df_price_response.shape[0]} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save price_response_grid.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_response.to_csv(GRID_PATH, index=False)\n",
    "print(f\"Price response grid saved to {GRID_PATH}\")\n",
    "df_price_response.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visual check: Plot 2-3 sample slots' revenue curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a few unique slot_ids from the grid\n",
    "if not df_price_response.empty:\n",
    "    sample_slot_ids = df_price_response['slot_id_test_set_idx'].unique()\n",
    "    \n",
    "    num_plots = min(3, len(sample_slot_ids))\n",
    "    if num_plots > 0:\n",
    "        plot_ids = np.random.choice(sample_slot_ids, size=num_plots, replace=False)\n",
    "\n",
    "        fig, axes = plt.subplots(num_plots, 1, figsize=(10, 5 * num_plots), sharex=True)\n",
    "        if num_plots == 1: axes = [axes] # Make it iterable\n",
    "\n",
    "        for i, slot_id in enumerate(plot_ids):\n",
    "            slot_data = df_price_response[df_price_response['slot_id_test_set_idx'] == slot_id]\n",
    "            ax = axes[i]\n",
    "            ax.plot(slot_data['simulated_price'], slot_data['expected_revenue'], marker='o', linestyle='-')\n",
    "            \n",
    "            # Find peak revenue\n",
    "            if not slot_data.empty:\n",
    "                peak_rev_point = slot_data.loc[slot_data['expected_revenue'].idxmax()]\n",
    "                ax.scatter(peak_rev_point['simulated_price'], peak_rev_point['expected_revenue'], color='red', s=100, zorder=5, \n",
    "                           label=f\"Peak Rev: £{peak_rev_point['expected_revenue']:.2f} @ £{peak_rev_point['simulated_price']:.2f}\")\n",
    "                ax.legend()\n",
    "\n",
    "            ax.set_ylabel(\"Expected Revenue (£)\")\n",
    "            # Include more info in title by fetching from the first row of slot_data\n",
    "            slot_info_row = slot_data.iloc[0]\n",
    "            title = (f\"Slot: {slot_id} (Venue: {slot_info_row['Venue Name']}, Date: {slot_info_row['search_date_for'].strftime('%Y-%m-%d')}, Hour: {slot_info_row['search_hour_for']})\\n\"\n",
    "                     f\"Cluster: {slot_info_row['km_cluster_label']}, Cap: {slot_info_row['capacity']}\")\n",
    "            ax.set_title(title)\n",
    "            ax.grid(True)\n",
    "\n",
    "        axes[-1].set_xlabel(\"Simulated Price (£)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(MODELS_DIR,\"sample_revenue_curves.png\"))\n",
    "        plt.show()\n",
    "        print(\"Sample revenue curves plotted and saved.\")\n",
    "    else:\n",
    "        print(\"Not enough unique slots in price response grid to plot.\")\n",
    "else:\n",
    "    print(\"Price response grid is empty, skipping plot.\")\n",
    "\n",
    "print(\"\\n--- Notebook execution complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
