


# 0. Import Libraries

import pandas as pd
from ydata_profiling import ProfileReport





# 1. Read the CSV into a DataFrame with explicit dtypes
dtype_map = {
    'Context ID': str,
    'Booking ID': str,
    'Session ID': str,
    'Search Days Ahead': 'Int64',
    'Search Charge': 'float',
    'Search Charge Type': 'category',
    'Venue ID': str,
    'Venue Name': 'category',
    'Party Size': 'Int64',
    'Was Search Available': 'boolean',
    'Reservation Days Ahead': 'Int64',
    'Reservation Charge': 'float',
    'Reservation Charge Type': 'category',
    'Year': 'Int64',
    'Month': 'Int64',
    'Reservation Cost ($)': 'float',
    'Packages Cost ($)': 'float',
    'Add Ons Cost ($)': 'float',
    'Promo Code Discount ($)': 'float',
    'Total Cost ($)': 'float',
    'Deposit Amount': 'float',
}
df = pd.read_csv(
    '../data/raw/Clays_data.csv',
    dtype=dtype_map,
    encoding='latin1',
    low_memory=False
)

# 2. Safety copy
df.to_parquet('../data/raw/full_raw.parquet', index=False)

# 3. Standardize date/time columns
df['Search At'] = pd.to_datetime(df['Search At'], dayfirst=True, errors='coerce')
df['Search Date'] = pd.to_datetime(df['Search Date'], dayfirst=True, errors='coerce')
df['Reservation Date'] = pd.to_datetime(df['Reservation Date'], dayfirst=True, errors='coerce')
df['Reservation Datetime'] = pd.to_datetime(df['Reservation Datetime'], dayfirst=True, errors='coerce')

# 4. Drop rows without Context ID (must be a bug)
df = df.dropna(subset=['Context ID'])

# 5. Save cleaned DataFrame
df.to_parquet('../data/processed/full_cleaned.parquet', index=False)

df






# 2. Clean
df = pd.read_parquet("../data/processed/full_cleaned.parquet")

# Drop invalid party sizes: <= 0 or > 20
mask_party = (df['Party Size'] > 0) & (df['Party Size'] <= 20)

# Remove negative days ahead and cap at 99th percentile (or 180 days)
# First drop negative values
df = df[ df['Search Days Ahead'] >= 0 ]

# Compute 99th percentile
pct_99 = df['Search Days Ahead'].quantile(0.99)
# Use guardrail of 180 days or computed pct, whichever is smaller
cap_days = min(180, pct_99)
#    Cap values
df['Search Days Ahead'] = df['Search Days Ahead'].clip(upper=cap_days)

# Drop rows outside party-size mask
df = df[ mask_party ]

# Remove negative money columns and absurd values
money_cols = [
    'Search Charge', 'Reservation Charge',
    'Reservation Cost ($)', 'Packages Cost ($)',
    'Add Ons Cost ($)', 'Promo Code Discount ($)',
    'Total Cost ($)', 'Deposit Amount'
]
for col in money_cols:
    # drop or set to NaN? here, i think we drop rows with negative
    df = df[df[col] >= 0]

# Reset index
df = df.reset_index(drop=True)

# Write out filtered dataset
df.to_parquet("../data/processed/mid_step_filtered.parquet", index=False)

print(f"Business-rule filtering complete. Rows now: {len(df)}")







# 3. Feature Engineering
def add_feature_flags(
    input_path: str = "../data/processed/mid_step_filtered.parquet",
    output_path: str = "../data/processed/master.parquet"
):
    # 1. Load filtered data
    df = pd.read_parquet(input_path)
    
    # 2. was_booked: 1 if there's a Reservation ID, else 0
    df["was_booked"] = df["Reservation ID"].notnull().astype("int8")
    
    # 3. lead_time_days = (reservation_date − search_date).days
    df["lead_time_days"] = (
        df["Reservation Date"] - df["Search Date"]
    ).dt.days.astype("Int16")
    # Drop any negative lead times that remain
    df = df[df["lead_time_days"] >= 0]
    
    # 4. Extract search-time features
    df["hour_of_day"] = pd.to_datetime(df["Search Time"]).dt.hour
    
    # undecided how to do these:
    #   df["is_weekend"] = df["day_of_week"].isin([5, 6]).astype("boolean")
    #   df["day_of_week"] = df["Search At"].dt.dayofweek.astype("Int8")  # Monday=0

    
    # 5. Convert business columns to categorical
    for col in [
        "Venue Name",
        "Search Charge Type",
        "Reservation Charge Type",
        "Booking Status",
    ]:
        if col in df:
            df[col] = df[col].astype("category")
    
    # 6. Persist the feature‐augmented dataset
    df.to_parquet(output_path, index=False)
    print(f"✅ Features added & saved to {output_path}")

if __name__ == "__main__":
    add_feature_flags()





# 4. Load
df2 = pd.read_parquet('../data/processed/master.parquet')
print(df2.info())
df2.head()







# Export HTML summary as data_quality.html

def generate_data_quality_report(
    input_path: str = "../data/processed/master.parquet",
    output_path: str = "../outputs/data_quality.html"
):
    # 1. Load your feature-augmented data
    df = pd.read_parquet(input_path)
    
    # 2. Create a profiling report
    profile = ProfileReport(
        df,
        title="Clays Data Quality Report",
        explorative=True,
        minimal=False  # set True for a slimmer report
    )
    
    # 3. Export to HTML
    profile.to_file(output_path)
    print(f"✅ Data-quality report written to {output_path}")

if __name__ == "__main__":
    generate_data_quality_report()
